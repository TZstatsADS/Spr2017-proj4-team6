---
title: "Script to test feature extraction"
output:
  pdf_document: default
  html_notebook: default
---

### Step 0: Setup project directory and dependencies
First we have to construct a citation vector from the co-author column of our dataset
```{r "setup", include=FALSE}
packages.used=c("knitr", "tidyr", "tidytext",
                "kernlab", "dplyr", "readr",
                "tm", "tibble", "stringr", "qlcMatrix")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

library(knitr)
library(tidyr)
library(tidytext)
library(kernlab)
library(dplyr)
library(readr)
library(tm)
library(tibble)
library(stringr)
library(qlcMatrix)

opts_knit$set(root.dir = normalizePath(".."))
projDir <- opts_knit$get("root.dir")
projDir
```

### Step 1: Create document term matrix
We use the `tm` package to create a corpus of the title strings.
```{r include=FALSE}
source(file.path(projDir,'lib','feature_extraction.R'))
files <- list.files(file.path(projDir,"output"), pattern = "*.csv")

exFilePath <- file.path(projDir,'output','AKumar.csv')
text <- readTextFile(exFilePath)
authorId <- text$AuthorID
dtm <- createCitationMatrix(text)
dtm_train_tfidf <- weightTfIdf(dtm,normalize = FALSE)
```

### Step 2.1: Run teacher's clustering algorithm from `kernlab` package
```{r, include=FALSE}
start.time <- Sys.time()
result_sclust <- specc(as.matrix(dtm_train_tfidf), 
                       centers=length(unique(authorId)))
end.time <- Sys.time()
time_sclust <- end.time - start.time


```

### Step 2.2: Run equivalent spectral clustering using a Gaussian-similarity-kernel and k-means clustering.
Here are the details of the implementation:

First, we compute the simmilarity between citations from the TF-IDF or NTF matrix of citations. We use a Gaussian kernel as a measure of similarity. Then, we create an undirected graph based on the similarities to extract some manifold in the data, we thereby obtain $A$, the affinity matrix. After, we calculate the degree matrix $D$ (diagonal) where each diagonal value is the degree of the respective vertex (*e.g.* sum of rows).
We compute the unnormalized graph Laplacian: $$U=D-A$$
Then, assuming that we want $k$ clusters, we find the $k$ smallest eigenvectors of $U$. This represents the points in a new $k$-dimensional space with low-variance.
Finally, in this transformed space, it becomes easy for a standard k-means clustering to find the appropriate clusters.

```{r, include=FALSE}
source(file.path(projDir,"lib","SpectralClustering.R"))
start.time <- Sys.time()
result_sKMclust <- specClusteringKM(as.matrix(dtm_train_tfidf), 
                       k=length(unique(authorId)))
end.time <- Sys.time()
time_sKMclust <- end.time - start.time
```

### Step 3: Run our spectral clustering algorithm in `lib/SpectralClustering.R`
Here are the details of the implementation:

From the TF-IDF or NTF matrix of citations, we compute the cosine similarity between each citation vectors as follows:
$$
similarity = cos(\theta) = \frac{a \cdot b}{\begin{Vmatrix}a\end{Vmatrix} \begin{Vmatrix}b\end{Vmatrix}} = \frac{\sum_{i=1}^n a_i b_i}{\sqrt{\sum_{i=1}^n a_i^2} \sqrt{\sum_{i=1}^n b_i^2}}
$$
This matrix is called the **Gram matrix** $A$. In the first step of the algorithm, we determine the $k$ largest eigenvectors of $A$: $X_k$, a $n$-by-$k$ matrix. Each row of $X_k$ corresponds to a citation vector.
Then, we compute the **QR decomposition with column pivoting** applied to ${X_k}^T$, *e.g.* we find the matrices $P$ (permutation matrix, $n$-by-$n$), $Q$ (orthogonal, $k$-by-$k$) and R (left-upper-triangular, $k$-by-$n$), so that:
$${X_k}^T P = QR = Q[R_{11}, R_{12}]$$
$R_{11}$ will be the $k$-by-$k$ upper-triangular matrix. We then compute the matrix $\hat{R}$:
$$\hat{R}=R_{11}^{-1}RP^T=R_{11}^{-1}[R_{11},R_{12}]P^T = [I_k,R_{11}^{-1}R_{12}]P^T$$
Finally, the cluster membership of each citation vector is determined by the row index of the largest element in absolute value of the corresponding column of $\hat{R}$.
```{r include=FALSE}
start.time <- Sys.time()
docsdissim <- cosSparse(t(as.matrix(dtm_train_tfidf)))
## apply K-WAY SPECTRAL CLUSTERING
k <- length(unique(authorId))
result_sQRclust <- specClusteringQR(as.matrix(docsdissim), k)
end.time <- Sys.time()
time_sQRclust <- end.time - start.time
```

### Step 4: Run evaluation metrics on both methods and compare
```{r}
source(file.path(projDir,"lib",'evaluation_measures.R'))
matching_matrix_sclust <- matching_matrix(authorId,result_sclust)
matching_matrix_sQRclust <- matching_matrix(authorId, result_sQRclust)
matching_matrix_sKMclust <- matching_matrix(authorId, result_sKMclust)
performance_sclust <- performance_statistics(matching_matrix_sclust)
performance_sQRclust <- performance_statistics(matching_matrix_sQRclust)
performance_sKMclust <- performance_statistics(matching_matrix_sKMclust)
```

```{r}
compare_df <- data.frame(method=c("Teacher","QR Spec.C", "Kmeans Spec.C"),
                         precision=c(performance_sclust$precision, performance_sQRclust$precision, performance_sKMclust$precision),
                         recall=c(performance_sclust$recall, performance_sQRclust$recall, performance_sKMclust$recall),
                         f1=c(performance_sclust$f1, performance_sQRclust$f1, performance_sKMclust$f1),
                         accuracy=c(performance_sclust$accuracy, performance_sQRclust$accuracy, performance_sKMclust$accuracy),
                         mcc=c(performance_sclust$mcc, performance_sQRclust$mcc, performance_sKMclust$mcc),
                         time=c(time_sclust, time_sQRclust, time_sKMclust))
kable(compare_df,caption="Comparision of performance for two clustering methods",digits = 2)
```

```{r, eval=FALSE, include=FALSE}
files <- list.files(file.path(projDir,"output"), pattern = "*.csv")

n <- length(files)
df<-tibble()
  
  #row.names = c("Precision", "Recall", "Accuracy")) # x='string',y=1:10,z='string'


for(i in 1:n){
  x <- runAuthorStudyDummy(files[i])
  print(paste(x[1], x[2], x[3]))
  add_row(df, accuracy=x[1], precision=x[2], recall=x[3])
}


```

```{r, eval=FALSE, include=FALSE}
df <- tibble(x = 1:3, y = 3:1)
dim(df)
df[3,1]="1231"

```

```{r, eval=FALSE, include=FALSE}

data.frame([], row.names = c("Precision", "Recall", "Accuracy"))
r <- 3
newrow <- seq(4)
insertRow <- function(existingDF, newrow, r) {
  existingDF[seq(r+1,nrow(existingDF)+1),] <- existingDF[seq(r,nrow(existingDF)),]
  existingDF[r,] <- newrow
  existingDF
}

```
